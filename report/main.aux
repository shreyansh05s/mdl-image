\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Krizhevsky09learningmultiple}
\citation{DBLP:journals/corr/abs-2010-11929}
\citation{DBLP:journals/corr/abs-2010-11929}
\citation{DBLP:journals/corr/abs-2103-14030}
\citation{DBLP:journals/corr/abs-2103-14030}
\citation{wandb}
\newlabel{fig:foobar}{{1}{2}{Illustration provided by \cite {DBLP:journals/corr/abs-2010-11929} represents the architecture of VIT}{figure.1}{}}
\citation{Kingma2014AdamAM}
\citation{DBLP:journals/corr/abs-2102-11600}
\newlabel{fig:foobar}{{2}{3}{Illustration provided by \cite {DBLP:journals/corr/abs-2103-14030} represents how SWIN architecture is different from VIT}{figure.2}{}}
\newlabel{tab:actor_critic_hp}{{1}{3}{Hyperparameters that were selected after running multiple iterations}{table.1}{}}
\newlabel{fig:foobar}{{3}{3}{Comparing our selected hyperparameters for VIT-ADAM and VIT-ASAM. This result itself is biased since VIT-ADAM pretrained already existed the goal of hyperparameter tuning was to find the best parameters for ASAM}{figure.3}{}}
\newlabel{fig:foobar}{{4}{4}{A visualization of how a Cosine Annealing LR works compared to a Step annealing LR scheduler}{figure.4}{}}
\citation{DBLP:journals/corr/abs-2102-11600}
\citation{DBLP:journals/corr/abs-2103-14030}
\citation{*}
\bibdata{main.bib}
\bibcite{paperswithcode}{{1}{}{{pap}}{{}}}
\bibcite{pytorch-cifar-model}{{2}{}{{pyt}}{{}}}
\bibcite{vit-pretrained}{{3}{}{{Ahmed9275}}{{}}}
\newlabel{fig:foobar}{{5}{5}{Visualization of loss decreasing over steps for VIT-ASAM and SWIN-ASAM-frozen. WE can see that VIT-ASAM has a lower loss value after 5 epochs}{figure.5}{}}
\newlabel{fig:foobar}{{6}{5}{Visualization of loss decreasing over steps for VIT-ASAM and SWIN-ASAM-frozen. We can see that VIT-ASAM has a lower loss value after 5 epochs and converges to the optima}{figure.6}{}}
\bibcite{wandb}{{4}{2020}{{Biewald}}{{}}}
\bibcite{deng2009imagenet}{{5}{2009}{{Deng et~al.}}{{Deng, Dong, Socher, Li, Li, and Fei-Fei}}}
\bibcite{DBLP:journals/corr/abs-2010-11929}{{6}{2020}{{Dosovitskiy et~al.}}{{Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby}}}
\bibcite{DBLP:journals/corr/abs-2010-01412}{{7}{2020}{{Foret et~al.}}{{Foret, Kleiner, Mobahi, and Neyshabur}}}
\bibcite{he2016deep}{{8}{2016}{{He et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{cifar-100-leaderboard}{{9}{}{{Huggingface}}{{}}}
\bibcite{Kingma2014AdamAM}{{10}{2014}{{Kingma \& Ba}}{{Kingma and Ba}}}
\bibcite{Krizhevsky09learningmultiple}{{11}{2009}{{Krizhevsky}}{{}}}
\bibcite{DBLP:journals/corr/abs-2102-11600}{{12}{2021}{{Kwon et~al.}}{{Kwon, Kim, Park, and Choi}}}
\bibcite{DBLP:journals/corr/abs-2103-14030}{{13}{2021}{{Liu et~al.}}{{Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and Guo}}}
\bibcite{swin-pretrained}{{14}{}{{MazenAmria}}{{}}}
\bibcite{NEURIPS2019_9015}{{15}{2019}{{Paszke et~al.}}{{Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala}}}
\bibcite{wolf-etal-2020-transformers}{{16}{2020}{{Wolf et~al.}}{{Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu, Scao, Gugger, Drame, Lhoest, and Rush}}}
\bibcite{wu2020visual}{{17}{2020}{{Wu et~al.}}{{Wu, Xu, Dai, Wan, Zhang, Yan, Tomizuka, Gonzalez, Keutzer, and Vajda}}}
\bibstyle{icml2021}
\gdef \@abspage@last{6}
